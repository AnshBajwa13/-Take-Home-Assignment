{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b7cffb-32cd-40ba-9ab1-f9531352b710",
   "metadata": {},
   "source": [
    " Part 1 preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2c57c-4039-4a8b-9a14-385cc04f0fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile  # For working with ZIP archives\n",
    "import os  # For interacting with the operating system (file paths)\n",
    "import re  # For regular expressions (text cleaning)\n",
    "import shutil  # For high-level file operations (deleting directories)\n",
    "from bs4 import BeautifulSoup  # For parsing HTML and XML\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text (handles HTML, CSS, and plain text).\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned text string.\n",
    "    \"\"\"\n",
    "    # Remove HTML/CSS tags using Beautiful Soup with lxml parser\n",
    "    soup = BeautifulSoup(text, \"lxml\")  # Create a BeautifulSoup object to parse HTML/CSS\n",
    "    text = soup.get_text(separator=\" \", strip=True)  # Extract text, add spaces between elements, remove leading/trailing whitespace\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation using regular expression\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(zip_file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses data from the zip file.\n",
    "\n",
    "    Args:\n",
    "        zip_file_path: The path to the ZIP file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are file types and values are lists of\n",
    "        preprocessed text strings.\n",
    "    \"\"\"\n",
    "    data = {}  # Initialize an empty dictionary to store the processed data\n",
    "\n",
    "    # Open the ZIP file in read mode using a 'with' statement (ensures proper closing)\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('temp_data')  # Extract all contents to a temporary 'temp_data' directory\n",
    "\n",
    "    # Assume the ZIP file contains a single inner folder\n",
    "    inner_folder = os.listdir('temp_data')[0]  # Get the name of the inner folder\n",
    "    print(f\"Inner folder found: {inner_folder}\")\n",
    "\n",
    "    # Iterate through each file in the inner folder\n",
    "    for filename in os.listdir(os.path.join('temp_data', inner_folder)):\n",
    "        if filename.endswith(\".txt\"):  # Check if the file is a text file\n",
    "            file_type = filename.split(\"_\")[0]  # Extract file type from filename (before the first underscore)\n",
    "            try:\n",
    "                # Open the text file in read mode with UTF-8 encoding\n",
    "                with open(os.path.join('temp_data', inner_folder, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()  # Read the entire file content into the 'text' variable\n",
    "                    processed_text = preprocess_text(text)  # Clean the text using the preprocess_text function\n",
    "\n",
    "                    # Organize the processed text by file type in the 'data' dictionary\n",
    "                    if file_type not in data:\n",
    "                        data[file_type] = []  # Create a new list for the file type if it doesn't exist\n",
    "                    data[file_type].append(processed_text)  # Add the processed text to the list\n",
    "\n",
    "                    print(f\"  Loaded file: {filename}\")\n",
    "                    print(f\"  Processed text (first 100 chars): {processed_text[:100]}...\")\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"UnicodeDecodeError while reading {filename}. Skipping this file.\")\n",
    "\n",
    "    # Delete the temporary directory after processing\n",
    "    try:\n",
    "        shutil.rmtree('temp_data')  # Delete the 'temp_data' directory and its contents\n",
    "    except PermissionError:\n",
    "        print(\"PermissionError: Could not delete 'temp_data' directory. Please close any open files and try again.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting 'temp_data' directory: {e}\")\n",
    "\n",
    "    print(f\"  Loaded data: {data}\")  # Print the contents of the 'data' dictionary (for debugging)\n",
    "    return data  # Return the dictionary containing the processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485916c-ff92-43fa-a005-9cea8b50e572",
   "metadata": {},
   "source": [
    "Part 2 extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213937e1-1303-46d6-b36e-a011c8c99707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (if not already loaded)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_topics(text, delay_seconds):\n",
    "    \"\"\"\n",
    "    Extracts topics from the given text using the ChatGPT API,\n",
    "    guided by key themes and seed topics.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting topics from text: {text[:50]}...\")  # Print the first 50 characters of the input text\n",
    "\n",
    "    # Define key themes (for focusing the topic extraction)\n",
    "    key_themes = [\n",
    "        \"user experience\",\n",
    "        \"technical issues\",\n",
    "        \"personalization\",\n",
    "        \"accessibility\",\n",
    "        \"peak hours\",\n",
    "        \"customer support\"\n",
    "    ]\n",
    "\n",
    "    # Define seed topics (for context)\n",
    "    seed_topics = [\n",
    "        \"teen therapy\",\n",
    "        \"group therapy\",\n",
    "        \"session notes\",\n",
    "        \"teletherapy audio\",\n",
    "        \"crisis response\",\n",
    "        \"user onboarding\",\n",
    "        \"video quality\",\n",
    "        \"appointment scheduling\",\n",
    "        \"therapy companion app\",\n",
    "        \"payment system\",\n",
    "        \"cultural matching\",\n",
    "        \"patient engagement\",\n",
    "        \"therapist matching\"\n",
    "    ]\n",
    "\n",
    "    # Construct the prompt for the ChatGPT API\n",
    "    prompt = f\"\"\"\n",
    "    Extract the key themes and topics discussed in the following text,\n",
    "    focusing on aspects like {\", \".join(key_themes)} and\n",
    "    considering the context of {\", \".join(seed_topics)}:\n",
    "\n",
    "    Text: \\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        client = openai.OpenAI()  # Create an instance of the OpenAI client\n",
    "        time.sleep(delay_seconds)  # Introduce a delay before each API call to avoid rate limits\n",
    "\n",
    "        for attempt in range(5):  # Retry up to 5 times in case of rate limit errors\n",
    "            try:\n",
    "                # Make the API call to generate completions (extract topics)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",  # Specify the model to use\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a helpful assistant that extracts key topics from text.\"\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt}  # Pass the constructed prompt to the API\n",
    "                    ],\n",
    "                    temperature=0.7,  # Control the randomness of the output\n",
    "                    max_tokens=150,  # Limit the number of tokens in the response\n",
    "                    top_p=1.0,  # Parameters for nucleus sampling (control diversity)\n",
    "                    frequency_penalty=0.0,  # Control the repetition of tokens\n",
    "                    presence_penalty=0.0,  # Control the appearance of new tokens\n",
    "                )\n",
    "\n",
    "                # Extract the topics from the API response\n",
    "                raw_topics = response.choices[0].message.content.split('\\n')  # Split the response into lines\n",
    "                topics = [topic.strip() for topic in raw_topics if topic.strip()]  # Remove extra whitespace and empty lines\n",
    "                print(f\"  Extracted topics from OpenAI: {topics}\")\n",
    "                return topics  # Return the extracted topics\n",
    "\n",
    "            except openai.RateLimitError as e:\n",
    "                # Implement exponential backoff with jitter for rate limit errors\n",
    "                wait_time = (4 ** attempt) + random.uniform(0, 5)  # Calculate wait time with increasing base and random jitter\n",
    "                print(f\"Rate limit exceeded, waiting for {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)  # Wait for the calculated time\n",
    "\n",
    "        else:  # This 'else' block executes if the loop completes without a successful response\n",
    "            print(\"Failed after multiple retries.\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_topics: {e}\")\n",
    "        return []  # Return an empty list if any error occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2e5d1-6311-4f8c-8f0f-a496933d1b3b",
   "metadata": {},
   "source": [
    "Part 3 Sentiment Analysis and Data Point Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d094d1-d97f-4764-b7d2-5a38f8e76070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Import the regular expression module\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # Import the sentiment analysis tool\n",
    "\n",
    "# NLTK Downloads (you might need to uncomment these if you haven't downloaded them)\n",
    "# nltk.download('vader_lexicon')  # Download the VADER lexicon for sentiment analysis\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the given text using NLTK's VADER.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "        A string representing the sentiment: \"positive\", \"negative\", or \"neutral\".\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()  # Create a SentimentIntensityAnalyzer object\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']  # Get the compound sentiment score\n",
    "\n",
    "    # Classify the sentiment based on the compound score\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"  # Positive sentiment\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"  # Negative sentiment\n",
    "    else:\n",
    "        return \"neutral\"  # Neutral sentiment\n",
    "\n",
    "\n",
    "def extract_data_points(text):\n",
    "    \"\"\"\n",
    "    Extracts relevant data points (percentages, numbers, dates) from the text.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "        A list of extracted data points (strings).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_points = []  # Initialize an empty list to store data points\n",
    "\n",
    "        # Improved date pattern to capture complete dates\n",
    "        date_pattern = r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\"  # Regular expression for YYYY-MM-DD dates\n",
    "        data_points.extend(re.findall(date_pattern, text))  # Find all dates and add them to the list\n",
    "\n",
    "        # Percentage pattern (no change needed)\n",
    "        percentage_pattern = r\"(\\d+\\.?\\d*)%\"  # Regular expression for percentages\n",
    "        data_points.extend(re.findall(percentage_pattern, text))  # Find all percentages and add them to the list\n",
    "\n",
    "        # Number pattern (modified to avoid capturing parts of dates)\n",
    "        number_pattern = r\"(?<!\\d-)(?<!\\d)\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b(?!\\d)\"  # Regular expression for numbers\n",
    "        data_points.extend(re.findall(number_pattern, text))  # Find all numbers and add them to the list\n",
    "\n",
    "        # Remove duplicates by converting to a set and back to a list\n",
    "        data_points = list(set(data_points))\n",
    "\n",
    "        return data_points  # Return the list of extracted data points\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_data_points: {e}\")\n",
    "        return []  # Return an empty list if an error occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555d8b7-d409-46b6-870d-9590b27a309a",
   "metadata": {},
   "source": [
    "part 4 generating story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3153935-3b62-49af-9890-fb70386b33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile  # For working with zip files\n",
    "import os  # For interacting with the operating system\n",
    "import re  # For regular expressions\n",
    "import shutil  # For high-level file operations\n",
    "import nltk  # For natural language processing tasks\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # For sentiment analysis\n",
    "import openai  # For using the OpenAI API\n",
    "from dotenv import load_dotenv  # For loading environment variables\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('vader_lexicon')  # Download the lexicon for sentiment analysis\n",
    "nltk.download('punkt')  # Download the punkt sentence tokenizer\n",
    "nltk.download('averaged_perceptron_tagger')  # Download the part-of-speech tagger\n",
    "nltk.download('maxent_ne_chunker')  # Download the named entity recognition chunker\n",
    "nltk.download('words')  # Download the words corpus\n",
    "\n",
    "# Load environment variables (make sure you have a .env file with your OpenAI API key)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Set the API key for OpenAI\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text.\n",
    "\n",
    "    Args:\n",
    "        text: The raw text content to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation using regular expressions\n",
    "    text = text.lower()  # Convert the text to lowercase\n",
    "    # (Optional) Add stemming/lemmatization here using NLTK\n",
    "    # ... your stemming/lemmatization code ...\n",
    "    return text\n",
    "\n",
    "def load_and_preprocess_data(zip_file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses data from the zip file.\n",
    "\n",
    "    Args:\n",
    "        zip_file_path: Path to the zip file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are file types and values are lists of\n",
    "        preprocessed text content for each file of that type.\n",
    "    \"\"\"\n",
    "    data = {}  # Initialize an empty dictionary to store the data\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:  # Open the zip file in read mode\n",
    "        zip_ref.extractall('temp_data')  # Extract all files to a temporary directory\n",
    "\n",
    "    # Get the name of the inner folder\n",
    "    inner_folder = os.listdir('temp_data')[0]  # Assuming there's only one folder inside\n",
    "\n",
    "    # Access the files within the inner folder\n",
    "    for filename in os.listdir(os.path.join('temp_data', inner_folder)):  # Loop through files in the inner folder\n",
    "        if filename.endswith(\".txt\"):  # Check if the file is a text file\n",
    "            file_type = filename.split(\"_\")[0]  # Extract the file type from the filename\n",
    "            try:\n",
    "                with open(os.path.join('temp_data', inner_folder, filename), 'r', encoding='utf-8') as file:  # Open the file with UTF-8 encoding\n",
    "                    text = file.read()  # Read the content of the file\n",
    "                    processed_text = preprocess_text(text)  # Preprocess the text\n",
    "                    if file_type not in data:  # If the file type is not already in the dictionary\n",
    "                        data[file_type] = []  # Create a new list for that file type\n",
    "                    data[file_type].append(processed_text)  # Add the processed text to the list\n",
    "            except UnicodeDecodeError:  # Handle UnicodeDecodeError if it occurs\n",
    "                print(f\"UnicodeDecodeError while reading {filename}. Skipping this file.\")\n",
    "\n",
    "    # Clean up the temporary directory\n",
    "    try:\n",
    "        shutil.rmtree('temp_data')  # Remove the temporary directory and its contents\n",
    "    except PermissionError:\n",
    "        print(\"PermissionError: Could not delete 'temp_data' directory. Please close any open files and try again.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting 'temp_data' directory: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_topics(text):\n",
    "    \"\"\"\n",
    "    Extracts topics from the given text using the ChatGPT API,\n",
    "    guided by key themes and seed topics.\n",
    "    \"\"\"\n",
    "    # Key themes from our analysis\n",
    "    key_themes = [\n",
    "        \"user experience\", \"technical issues\", \"personalization\",\n",
    "        \"accessibility\", \"peak hours\", \"customer support\"\n",
    "    ]\n",
    "\n",
    "    # Seed topics (you'll need to customize these based on the specific file being analyzed)\n",
    "    seed_topics = [\n",
    "        \"teen therapy\", \"group therapy\", \"session notes\", \"teletherapy audio\",\n",
    "        \"crisis response\", \"user onboarding\", \"video quality\",\n",
    "        \"appointment scheduling\", \"therapy companion app\", \"payment system\",\n",
    "        \"cultural matching\", \"patient engagement\", \"therapist matching\"\n",
    "    ]\n",
    "\n",
    "    # Construct the prompt with key themes and seed topics\n",
    "    prompt = f\"\"\"\n",
    "    Extract the key themes and topics discussed in the following text, \n",
    "    focusing on aspects like {\", \".join(key_themes)} and \n",
    "    considering the context of {\", \".join(seed_topics)}:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(  # Make an API call to OpenAI's ChatCompletion\n",
    "            model=\"gpt-3.5-turbo\",  # Use the gpt-3.5-turbo model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts key topics from text.\"},  # Set the system message\n",
    "                {\"role\": \"user\", \"content\": prompt}  # Set the user message with the prompt\n",
    "            ]\n",
    "        )\n",
    "        topics = response['choices'][0]['message']['content'].split('\\n')  # Split the response into topics by newline\n",
    "        topics = [topic.strip() for topic in topics if topic.strip()]  # Remove leading/trailing whitespace from topics\n",
    "        return topics\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_topics: {e}\")  # Print any errors that occur during topic extraction\n",
    "        return []\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the given text.\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()  # Create a SentimentIntensityAnalyzer object\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']  # Get the compound sentiment score\n",
    "    if sentiment_score >= 0.05:  # Classify sentiment based on the score\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def extract_data_points(text):\n",
    "    \"\"\"\n",
    "    Extracts relevant data points (percentages, numbers, dates) from the text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_points = []  # Initialize an empty list to store data points\n",
    "\n",
    "        # Find percentages\n",
    "        percentage_pattern = r\"(\\d+\\.?\\d*)%\"  # Regular expression pattern to find percentages\n",
    "        data_points.extend(re.findall(percentage_pattern, text))  # Find all percentages in the text\n",
    "\n",
    "        # Find numbers (integers and decimals)\n",
    "        number_pattern = r\"(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\"  # Regular expression pattern to find numbers\n",
    "        data_points.extend(re.findall(number_pattern, text))  # Find all numbers in the text\n",
    "\n",
    "        # Find dates (adjust the pattern based on your data)\n",
    "        date_pattern = r\"(\\d{4}-\\d{2}-\\d{2})\"  # Regular expression pattern to find dates (YYYY-MM-DD)\n",
    "        data_points.extend(re.findall(date_pattern, text))  # Find all dates in the text\n",
    "\n",
    "        return data_points  # Return the list of data points\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_data_points: {e}\")  # Print any errors that occur during data point extraction\n",
    "        return []\n",
    "\n",
    "def generate_summary_with_chatgpt(topics, sentiment, data_points, references):\n",
    "    \"\"\"\n",
    "    Generates a summary using the ChatGPT API, focusing on key themes and data points.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()  # Create an OpenAI API client\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Generate a concise summary for a product story, based on the following information:\n",
    "\n",
    "        Extracted Topics: {\", \".join(topics)}\n",
    "\n",
    "        Sentiment: {sentiment}\n",
    "\n",
    "        Data Points: {\", \".join(data_points)}\n",
    "\n",
    "        References: {\", \".join(references)}\n",
    "\n",
    "        The summary should highlight key insights, user experiences, and relevant data points.\n",
    "        Focus on clarity and conciseness.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(  # Make an API call to OpenAI's ChatCompletion\n",
    "            model=\"gpt-3.5-turbo\",  # Use the gpt-3.5-turbo model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates product story summaries.\"},  # Set the system message\n",
    "                {\"role\": \"user\", \"content\": prompt}  # Set the user message with the prompt\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=250,  # Adjust as needed for summary length\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "        )\n",
    "\n",
    "        summary = response.choices[0].message.content.strip()  # Extract the summary from the response\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_summary_with_chatgpt: {e}\")  # Print any errors that occur during summary generation\n",
    "        return None\n",
    "\n",
    "def generate_story(story_type, topics, sentiment, data_points, references, text):\n",
    "    \"\"\"\n",
    "    Generates a product story based on the given information and detailed templates.\n",
    "    \"\"\"\n",
    "    print(f\"      Generating story of type: {story_type}\")\n",
    "\n",
    "    # Define templates for different story types\n",
    "    templates = {\n",
    "        \"Concern\": {\n",
    "            \"headline_template\": \"[Problem/Issue] is Negatively Impacting [User/Area]\",\n",
    "            \"summary_template\": \"Analysis of {references} reveals concerns regarding {topics}. {data_points} indicate a negative impact on {user_group}.\",\n",
    "            \"impact_template\": \"This issue has led to {impact}.\",\n",
    "            \"user_perspective_template\": \"Users have expressed {user_perspectives} about this problem.\",\n",
    "            \"recommendation_template\": \"To address this concern, it is recommended to {recommendations}.\",\n",
    "        },\n",
    "        \"Win\": {\n",
    "            \"headline_template\": \"[Positive Outcome/Achievement] in [Area/Feature]\",\n",
    "            \"summary_template\": \"{data_points} highlight the success of {feature} in {references}. User feedback indicates {positive_feedback}.\",\n",
    "            \"contributing_factors_template\": \"This success can be attributed to {factors}.\",\n",
    "            \"impact_template\": \"This positive outcome has resulted in {impact}.\",\n",
    "            \"user_perspective_template\": \"Users have expressed {user_perspectives} about this feature.\",\n",
    "            \"recommendation_template\": \"To maintain and further enhance this success, it is recommended to {recommendations}.\"\n",
    "        },\n",
    "        \"Insight\": {\n",
    "            \"headline_template\": \"Key Insight: [New Understanding/Discovery] in [Area/Feature]\",\n",
    "            \"summary_template\": \"Analysis of {references} reveals a key insight: {insight}. This has implications for {area}.\",\n",
    "            \"evidence_template\": \"This insight is supported by the following evidence: {evidence}.\",\n",
    "            \"implication_template\": \"This insight suggests that {implication}.\",\n",
    "            \"recommendation_template\": \"We recommend {recommendations} to leverage this insight.\"\n",
    "        },\n",
    "        \"Opportunity\": {\n",
    "            \"headline_template\": \"Opportunity to [Improve/Innovate] in [Area/Feature]\",\n",
    "            \"summary_template\": \"{references} highlight an opportunity to {action} in {area}. This could lead to {benefits}.\",\n",
    "            \"potential_template\": \"This opportunity has the potential to {potential}.\",\n",
    "            \"recommendation_template\": \"To capitalize on this opportunity, it is recommended to {recommendations}.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    template = templates.get(story_type)  # Get the template for the specified story type\n",
    "    if not template:\n",
    "        return \"Error: Invalid story type.\"  # Return an error message if the story type is invalid\n",
    "\n",
    "    # Generate summary using ChatGPT\n",
    "    summary = generate_summary_with_chatgpt(text, topics, data_points, references)  # Generate a summary using the ChatGPT API\n",
    "    if summary is None:  # If summary generation fails\n",
    "        summary = f\"Analysis of {', '.join(references)} reveals {'concerns regarding' if sentiment == 'negative' else 'insights into'} {', '.join(topics)}.\"  # Create a default summary\n",
    "\n",
    "    # Placeholder values (you might want to refine these based on your analysis)\n",
    "    placeholder_values = {\n",
    "        \"Concern\": {\n",
    "            \"impact\": \"increased frustration and decreased engagement among users\",\n",
    "            \"user_perspectives\": \"'The app is slow and buggy,' and 'I'm having trouble finding the information I need'\",\n",
    "            \"recommendations\": \"optimize the app's performance, improve navigation, and provide clearer instructions\"\n",
    "        },\n",
    "        \"Win\": {\n",
    "            \"factors\": \"the user-friendly interface, personalized content, and effective communication tools\",\n",
    "            \"impact\": \"increased user satisfaction, higher engagement rates, and improved clinical outcomes\",\n",
    "            \"user_perspectives\": \"'The new onboarding process is much smoother and more intuitive,' and 'I feel like the app really understands my needs now'\",\n",
    "            \"recommendations\": \"continue gathering user feedback, expand the platform's reach, and explore new features based on user needs\"\n",
    "        },\n",
    "        \"Insight\": {\n",
    "            \"evidence\": \"Data from product_metrics_3.txt shows an 85% client retention rate and a 92% session completion rate for teen therapy.\",\n",
    "            \"implication\": \"Teenagers are effectively engaging with the platform and finding value in the services offered.\",\n",
    "            \"recommendations\": \"Continue to monitor these metrics and gather feedback to ensure ongoing satisfaction and engagement among teen users.\"\n",
    "        },\n",
    "        \"Opportunity\": {\n",
    "            \"potential\": \"significantly enhance the user experience, improve accessibility, and potentially increase user engagement and retention.\",\n",
    "            \"recommendations\": \"conduct user research to gather more specific feedback, explore potential design solutions, and prioritize development based on user needs and impact.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Generate the story using the template and extracted information\n",
    "    story = f\"\"\"\n",
    "    ## {template['headline_template'].format(Problem=topics[0] if topics else \"users\", User='users', Area=topics[0] if topics else \"users\")}\n",
    "\n",
    "    Summary: {summary}\n",
    "\n",
    "    {template.get('impact_template', '').format(impact=placeholder_values[story_type]['impact'])}\n",
    "    {template.get('user_perspective_template', '').format(user_perspectives=placeholder_values[story_type]['user_perspectives'])}\n",
    "    {template.get('contributing_factors_template', '').format(factors=placeholder_values[story_type].get('factors', ''))}\n",
    "    {template.get('evidence_template', '').format(evidence=placeholder_values[story_type].get('evidence', ''))}\n",
    "    {template.get('implication_template', '').format(implication=placeholder_values[story_type].get('implication', ''))}\n",
    "    {template.get('potential_template', '').format(potential=placeholder_values[story_type].get('potential', ''))}\n",
    "    {template.get('recommendation_template', '').format(recommendations=placeholder_values[story_type]['recommendations'])}\n",
    "\n",
    "    **References:**\n",
    "    {\", \".join(references)}\n",
    "    \"\"\"\n",
    "    return story\n",
    "\n",
    "def generate_product_stories(preprocessed_data):\n",
    "    \"\"\"\n",
    "    Generates product stories for the given preprocessed data.\n",
    "    \"\"\"\n",
    "    all_stories = []\n",
    "    for file_type, files in preprocessed_data.items():\n",
    "        if file_type in [\n",
    "            \"feedback_analysis\",\n",
    "            \"interview_transcripts\",\n",
    "            \"product_metrics\",\n",
    "            \"product_intelligence_report\",\n",
    "            \"user_journey\",\n",
    "        ]:\n",
    "            for i, text in enumerate(files):\n",
    "                print(f\"Processing file: {file_type}_{i + 1}.txt\")\n",
    "                topics = extract_topics(text)\n",
    "                sentiment = analyze_sentiment(text)\n",
    "                data_points = extract_data_points(text)\n",
    "                print(f\"  Topics: {topics}\")\n",
    "                print(f\"  Sentiment: {sentiment}\")\n",
    "                print(f\"  Data points: {data_points}\")\n",
    "\n",
    "                # Basic story type classification logic\n",
    "                if sentiment == \"negative\":\n",
    "                    story_type = \"Concern\"\n",
    "                elif sentiment == \"positive\":\n",
    "                    story_type = \"Win\"\n",
    "                else:\n",
    "                    story_type = \"Insight\"  # Default\n",
    "\n",
    "                story = generate_story(\n",
    "                story_type,\n",
    "                topics,\n",
    "                sentiment,\n",
    "                data_points,\n",
    "                [f\"{file_type}_{i + 1}.txt\"],\n",
    "                text,\n",
    "                )\n",
    "                print(f\"  Generated story:\\n{story}\\n\")\n",
    "\n",
    "                all_stories.append(story)\n",
    "    return all_stories\n",
    "\n",
    "# Example usage\n",
    "zip_file_path = r'C:\\Users\\Anshd\\Downloads\\data\\take_home_data.zip'  # Replace with your actual zip file path\n",
    "preprocessed_data = load_and_preprocess_data(zip_file_path)\n",
    "stories = generate_product_stories(preprocessed_data)\n",
    "\n",
    "# Print the generated stories\n",
    "for i, story in enumerate(stories):\n",
    "    print(f\"--- Story {i+1} ---\\n{story}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d78ff-1503-4bd8-bc72-0d30427b3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing part 3 with sample test\n",
    "\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# NLTK Downloads (if not already downloaded)\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Performs sentiment analysis on the given text.\"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def extract_data_points(text):\n",
    "    \"\"\"Extracts relevant data points (percentages, numbers, dates) from the text.\"\"\"\n",
    "    try:\n",
    "        data_points = []\n",
    "\n",
    "        # Improved date pattern to capture complete dates\n",
    "        date_pattern = r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\"\n",
    "        data_points.extend(re.findall(date_pattern, text))\n",
    "\n",
    "        # Percentage pattern (no change needed)\n",
    "        percentage_pattern = r\"(\\d+\\.?\\d*)%\"\n",
    "        data_points.extend(re.findall(percentage_pattern, text))\n",
    "\n",
    "        # Number pattern (modified to avoid capturing parts of dates)\n",
    "        number_pattern = r\"(?<!\\d-)(?<!\\d)\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b(?!\\d)\"\n",
    "        data_points.extend(re.findall(number_pattern, text))\n",
    "\n",
    "        # Remove duplicates by converting to a set and back to a list\n",
    "        data_points = list(set(data_points))\n",
    "\n",
    "        return data_points\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_data_points: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example Usage for Testing:\n",
    "sample_text = \"\"\"\n",
    "This is a sample text with some data points. The user satisfaction rate is 95.5%.\n",
    "The app was launched on 2023-05-15. There are 1,234 active users. The feedback is positive.\n",
    "\"\"\"\n",
    "sentiment = analyze_sentiment(sample_text)\n",
    "data_points = extract_data_points(sample_text)\n",
    "\n",
    "print(f\"Sentiment for sample text: {sentiment}\")\n",
    "print(f\"Data points extracted: {data_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb7a06-569b-4ccf-8402-7a10592b1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code without summarisation\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import time\n",
    "import random\n",
    "\n",
    "# --- PART 1: Data Loading and Preprocessing ---\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# NLTK Downloads (you might need to uncomment these if you haven't downloaded them)\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses the given text (handles HTML, CSS, and plain text).\"\"\"\n",
    "    # Remove HTML/CSS tags using Beautiful Soup with lxml parser\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def load_and_preprocess_data(zip_file_path):\n",
    "    \"\"\"Loads and preprocesses data from the zip file.\"\"\"\n",
    "    data = {}\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('temp_data')\n",
    "\n",
    "    inner_folder = os.listdir('temp_data')[0]\n",
    "    print(f\"Inner folder found: {inner_folder}\")\n",
    "\n",
    "    for filename in os.listdir(os.path.join('temp_data', inner_folder)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_type = filename.split(\"_\")[0]\n",
    "            try:\n",
    "                with open(os.path.join('temp_data', inner_folder, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    processed_text = preprocess_text(text)\n",
    "                    if file_type not in data:\n",
    "                        data[file_type] = []\n",
    "                    data[file_type].append(processed_text)\n",
    "\n",
    "                    print(f\"  Loaded file: {filename}\")\n",
    "                    print(f\"  Processed text (first 100 chars): {processed_text[:100]}...\")\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"UnicodeDecodeError while reading {filename}. Skipping this file.\")\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('temp_data')\n",
    "    except PermissionError:\n",
    "        print(\"PermissionError: Could not delete 'temp_data' directory. Please close any open files and try again.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting 'temp_data' directory: {e}\")\n",
    "\n",
    "    print(f\"  Loaded data: {data}\")\n",
    "    return data\n",
    "\n",
    "# --- PART 2: Topic Extraction ---\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_topics(text, delay_seconds):\n",
    "    \"\"\"\n",
    "    Extracts topics from the given text using the ChatGPT API,\n",
    "    guided by key themes and seed topics.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting topics from text: {text[:50]}...\")\n",
    "\n",
    "    key_themes = [\n",
    "        \"user experience\", \"technical issues\", \"personalization\",\n",
    "        \"accessibility\", \"peak hours\", \"customer support\"\n",
    "    ]\n",
    "\n",
    "    seed_topics = [\n",
    "        \"teen therapy\", \"group therapy\", \"session notes\", \"teletherapy audio\",\n",
    "        \"crisis response\", \"user onboarding\", \"video quality\",\n",
    "        \"appointment scheduling\", \"therapy companion app\", \"payment system\",\n",
    "        \"cultural matching\", \"patient engagement\", \"therapist matching\"\n",
    "    ]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Extract the key themes and topics discussed in the following text,\n",
    "    focusing on aspects like {\", \".join(key_themes)} and\n",
    "    considering the context of {\", \".join(seed_topics)}:\n",
    "\n",
    "    Text: \\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        time.sleep(delay_seconds)  # Delay before each API call\n",
    "        for attempt in range(5):  # Retry up to 5 times\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",  # Or a cheaper model if available\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts key topics from text.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=150,\n",
    "                    top_p=1.0,\n",
    "                    frequency_penalty=0.0,\n",
    "                    presence_penalty=0.0,\n",
    "                )\n",
    "                raw_topics = response.choices[0].message.content.split('\\n')\n",
    "                topics = [topic.strip() for topic in raw_topics if topic.strip()]\n",
    "                print(f\"  Extracted topics from OpenAI: {topics}\")\n",
    "                return topics\n",
    "\n",
    "            except openai.RateLimitError as e:\n",
    "                wait_time = (4 ** attempt) + random.uniform(0, 5)  # Aggressive backoff\n",
    "                print(f\"Rate limit exceeded, waiting for {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed after multiple retries.\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_topics: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- PART 3: Sentiment Analysis and Data Point Extraction ---\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Performs sentiment analysis on the given text.\"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def extract_data_points(text):\n",
    "    \"\"\"Extracts relevant data points (percentages, numbers, dates) from the text.\"\"\"\n",
    "    try:\n",
    "        data_points = []\n",
    "\n",
    "        # Improved date pattern to capture complete dates\n",
    "        date_pattern = r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\"\n",
    "        data_points.extend(re.findall(date_pattern, text))\n",
    "\n",
    "        # Percentage pattern (no change needed)\n",
    "        percentage_pattern = r\"(\\d+\\.?\\d*)%\"\n",
    "        data_points.extend(re.findall(percentage_pattern, text))\n",
    "\n",
    "        # Number pattern (modified to avoid capturing parts of dates)\n",
    "        number_pattern = r\"(?<!\\d-)(?<!\\d)\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b(?!\\d)\"\n",
    "        data_points.extend(re.findall(number_pattern, text))\n",
    "\n",
    "        # Remove duplicates by converting to a set and back to a list\n",
    "        data_points = list(set(data_points))\n",
    "\n",
    "        return data_points\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_data_points: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- PART 4: Story Generation ---\n",
    "\n",
    "def generate_story(story_type, topics, sentiment, data_points, references, text):\n",
    "    \"\"\"\n",
    "    Generates a product story based on the given information and detailed templates.\n",
    "    \"\"\"\n",
    "    print(f\"      Generating story of type: {story_type}\")\n",
    "\n",
    "    # Define templates for different story types with additional sections\n",
    "    templates = {\n",
    "        \"Concern\": {\n",
    "            \"headline_template\": \"[Problem/Issue] is Negatively Impacting [User/Area]\",\n",
    "            \"summary_template\": \"Analysis of {references} reveals concerns regarding {topics}. {data_points} indicate a negative impact on {user_group}.\",\n",
    "            \"impact_template\": \"This issue has led to {impact}.\",\n",
    "            \"user_perspective_template\": \"Users have expressed {user_perspectives} about this problem.\",\n",
    "            \"recommendation_template\": \"To address this concern, it is recommended to {recommendations}.\",\n",
    "        },\n",
    "        \"Win\": {\n",
    "            \"headline_template\": \"[Positive Outcome/Achievement] in [Area/Feature]\",\n",
    "            \"summary_template\": \"{data_points} highlight the success of {feature} in {references}. User feedback indicates {positive_feedback}.\",\n",
    "            \"contributing_factors_template\": \"This success can be attributed to {factors}.\",\n",
    "            \"impact_template\": \"This positive outcome has resulted in {impact}.\",\n",
    "            \"user_perspective_template\": \"Users have expressed {user_perspectives} about this feature.\",\n",
    "            \"recommendation_template\": \"To maintain and further enhance this success, it is recommended to {recommendations}.\"\n",
    "        },\n",
    "        \"Insight\": {\n",
    "            \"headline_template\": \"Key Insight: [New Understanding/Discovery] in [Area/Feature]\",\n",
    "            \"summary_template\": \"Analysis of {references} reveals a key insight: {insight}. This has implications for {area}.\",\n",
    "            \"evidence_template\": \"This insight is supported by the following evidence: {evidence}.\",\n",
    "            \"implication_template\": \"This insight suggests that {implication}.\",\n",
    "            \"recommendation_template\": \"We recommend {recommendations} to leverage this insight.\"\n",
    "        },\n",
    "        \"Opportunity\": {\n",
    "            \"headline_template\": \"Opportunity to [Improve/Innovate] in [Area/Feature]\",\n",
    "            \"summary_template\": \"{references} highlight an opportunity to {action} in {area}. This could lead to {benefits}.\",\n",
    "            \"potential_template\": \"This opportunity has the potential to {potential}.\",\n",
    "            \"recommendation_template\": \"To capitalize on this opportunity, it is recommended to {recommendations}.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    template = templates.get(story_type)\n",
    "    if not template:\n",
    "        return \"Error: Invalid story type.\"\n",
    "\n",
    "    # --- Input for Placeholders based on our analysis ---\n",
    "\n",
    "    if story_type == \"Concern\":\n",
    "        impact = \"increased frustration and decreased engagement among users\"\n",
    "        user_perspectives = \"'The app is slow and buggy,' and 'I'm having trouble finding the information I need'\"\n",
    "        recommendations = \"optimize the app's performance, improve navigation, and provide clearer instructions\"\n",
    "    elif story_type == \"Win\":\n",
    "        factors = \"the user-friendly interface, personalized content, and effective communication tools\"\n",
    "        impact = \"increased user satisfaction, higher engagement rates, and improved clinical outcomes\"\n",
    "        user_perspectives = \"'The new onboarding process is much smoother and more intuitive,' and 'I feel like the app really understands my needs now'\"\n",
    "        recommendations = \"continue gathering user feedback, expand the platform's reach, and explore new features based on user needs\"\n",
    "    elif story_type == \"Insight\":\n",
    "        evidence = \"Data from product_metrics_3.txt shows an 85% client retention rate and a 92% session completion rate for teen therapy.\"\n",
    "        implication = \"Teenagers are effectively engaging with the platform and finding value in the services offered.\"\n",
    "        recommendations = \"Continue to monitor these metrics and gather feedback to ensure ongoing satisfaction and engagement among teen users.\"\n",
    "    elif story_type == \"Opportunity\":\n",
    "        potential = \"significantly enhance the user experience, improve accessibility, and potentially increase user engagement and retention.\"\n",
    "        recommendations = \"conduct user research to gather more specific feedback, explore potential design solutions, and prioritize development based on user needs and impact.\"\n",
    "\n",
    "    # Generate the story using the template and extracted information\n",
    "    story = f\"\"\"\n",
    "    ## {template['headline_template'].format(Problem=topics[0], User='users', Area=topics[0])}\n",
    "\n",
    "    {template['summary_template'].format(references=\", \".join(references), topics=\", \".join(topics), data_points=\", \".join(data_points), user_group=\"users\", feature=topics[0], insight=topics[0], area=topics[0], action=\"improve\", benefits=\"better user experience\")}\n",
    "\n",
    "    {template.get('impact_template', '').format(impact=impact)}\n",
    "    {template.get('user_perspective_template', '').format(user_perspectives=user_perspectives)}\n",
    "    {template.get('contributing_factors_template', '').format(factors=factors)}\n",
    "    {template.get('evidence_template', '').format(evidence=evidence)}\n",
    "    {template.get('implication_template', '').format(implication=implication)}\n",
    "    {template.get('potential_template', '').format(potential=potential)}\n",
    "    \n",
    "    {template['recommendation_template'].format(recommendations=recommendations)}\n",
    "\n",
    "    **References:**\n",
    "    {\", \".join(references)}\n",
    "    \"\"\"\n",
    "    return story\n",
    "\n",
    "def generate_product_stories(preprocessed_data):\n",
    "    \"\"\"\n",
    "    Generates product stories for the given preprocessed data.\n",
    "    \"\"\"\n",
    "    all_stories = []\n",
    "    for file_type, files in preprocessed_data.items():\n",
    "        if file_type in [\"feedback_analysis\", \"interview_transcripts\", \"product_metrics\", \"product_intelligence_report\", \"user_journey\"]:\n",
    "            for i, text in enumerate(files):\n",
    "                print(f\"Processing file: {file_type}_{i+1}.txt\")\n",
    "                topics = extract_topics(text)\n",
    "                sentiment = analyze_sentiment(text)\n",
    "                data_points = extract_data_points(text)\n",
    "\n",
    "                print(f\"  Topics: {topics}\")\n",
    "                print(f\"  Sentiment: {sentiment}\")\n",
    "                print(f\"  Data points: {data_points}\")\n",
    "\n",
    "                # Story type classification logic (add your rules here)\n",
    "                if sentiment == \"negative\":\n",
    "                    if any(topic in [\"technical issues\", \"connectivity problems\", \"errors\", \"delays\", \"sync issues\", \"audio quality\", \"video quality\", \"payment delays\"] for topic in topics):\n",
    "                        story_type = \"Concern\"\n",
    "                    elif any(topic in [\"accessibility issues\", \"limited availability\", \"mismatch\", \"long wait times\"] for topic in topics):\n",
    "                        story_type = \"Concern\"\n",
    "                    # ... (Add more rules for \"Concern\" based on your analysis) ...\n",
    "                elif sentiment == \"positive\":\n",
    "                    if any(topic in [\"user satisfaction\", \"clinical improvement\", \"positive feedback\", \"high engagement\", \"effective communication\", \"successful implementation\"] for topic in topics):\n",
    "                        story_type = \"Win\"\n",
    "                    elif any(topic in [\"ease of use\", \"intuitive interface\", \"personalized experience\", \"high adoption rate\"] for topic in topics):\n",
    "                        story_type = \"Win\"\n",
    "                    # ... (Add more rules for \"Win\" based on your analysis) ...\n",
    "                elif sentiment == \"neutral\":\n",
    "                    if any(topic in [\"user behavior\", \"new discovery\", \"insights\", \"trends\", \"peak hours\", \"user journey\"] for topic in topics):\n",
    "                        story_type = \"Insight\"\n",
    "                    # ... (Add more rules for \"Insight\" based on your analysis) ...\n",
    "                elif any(topic in [\"opportunity\", \"potential\", \"enhancement\", \"innovation\", \"future development\"] for topic in topics):\n",
    "                    story_type = \"Opportunity\"\n",
    "                else:\n",
    "                    story_type = \"Insight\"  # Default to \"Insight\" if no other rules match\n",
    "\n",
    "                story = generate_story(story_type, topics, sentiment, data_points, [f\"{file_type}_{i+1}.txt\"], text)\n",
    "                print(f\"  Generated story:\\n{story}\\n\")\n",
    "\n",
    "                all_stories.append(story)\n",
    "    return all_stories\n",
    "\n",
    "# --- INTEGRATION AND TESTING (Manual File Selection) ---\n",
    "\n",
    "# Example Usage (Integrating all parts with manual file selection):\n",
    "zip_file_path = r'C:\\Users\\Anshd\\Downloads\\data\\take_home_data.zip'  # Your zip file path\n",
    "preprocessed_data = load_and_preprocess_data(zip_file_path)\n",
    "\n",
    "# VERY long delay (e.g., 60 seconds or more) - adjust as needed\n",
    "delay_seconds = 60\n",
    "\n",
    "# Select a single file type and file index to process\n",
    "file_type_to_process = \"design\"  # Change this to process a different file type\n",
    "file_index_to_process = 0\n",
    "\n",
    "if file_type_to_process in preprocessed_data:\n",
    "    text_list = preprocessed_data[file_type_to_process]\n",
    "\n",
    "    if file_index_to_process < len(text_list):\n",
    "        text = text_list[file_index_to_process]\n",
    "        shortened_text = text[:500]  # Use only the first 500 characters for testing\n",
    "\n",
    "        # Part 2: Topic Extraction\n",
    "        topics = extract_topics(shortened_text, delay_seconds)\n",
    "        print(f\"  Topics extracted for {file_type_to_process}: {topics}\")\n",
    "\n",
    "        # Part 3: Sentiment Analysis and Data Point Extraction\n",
    "        sentiment = analyze_sentiment(shortened_text)\n",
    "        data_points = extract_data_points(shortened_text)\n",
    "\n",
    "        print(f\"  Sentiment for {file_type_to_process}: {sentiment}\")\n",
    "        print(f\"  Data points extracted for {file_type_to_process}: {data_points}\")\n",
    "\n",
    "        # Part 4: Story Generation\n",
    "        story = generate_story(\"Concern\", topics, sentiment, data_points, [f\"{file_type_to_process}_{file_index_to_process + 1}.txt\"], shortened_text)\n",
    "        print(f\"  Generated story:\\n{story}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"  No file found at index {file_index_to_process} for {file_type_to_process}\")\n",
    "else:\n",
    "    print(f\"  File type '{file_type_to_process}' not found in preprocessed data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320a83f-1173-47c9-8ad3-2a2cf0114a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
